2025-10-19 13:15:19 - tokenizer_training_rank_0 - INFO - === Tokenizer Training Started ===
2025-10-19 13:15:19 - tokenizer_training_rank_0 - INFO - Experiment Name: BTCUSDT_1h_finetune
2025-10-19 13:15:19 - tokenizer_training_rank_0 - INFO - Log Directory: /root/project/Kronos-Btc-finetune-master/Kronos/finetune_csv/finetuned//BTCUSDT_1h_finetune/logs
2025-10-19 13:15:19 - tokenizer_training_rank_0 - INFO - Rank: 0
2025-10-19 13:15:19 - tokenizer_training_rank_0 - INFO - Timestamp: 2025-10-19 13:15:19
2025-10-19 13:15:19 - tokenizer_training_rank_0 - INFO - Loading pretrained tokenizer...
2025-10-19 13:15:19 - tokenizer_training_rank_0 - INFO - Tokenizer parameters: 3,958,042
2025-10-19 13:15:19 - tokenizer_training_rank_0 - INFO - === Training Configuration ===
2025-10-19 13:15:19 - tokenizer_training_rank_0 - INFO - Data path: /root/project/Kronos-Btc-finetune-master/Kronos/finetune_csv/data/BTCUSDT_1h_20251018_220012_fixed.csv
2025-10-19 13:15:19 - tokenizer_training_rank_0 - INFO - Lookback window: 512
2025-10-19 13:15:19 - tokenizer_training_rank_0 - INFO - Predict window: 48
2025-10-19 13:15:19 - tokenizer_training_rank_0 - INFO - Batch size: 32
2025-10-19 13:15:19 - tokenizer_training_rank_0 - INFO - Learning rate: 0.0002
2025-10-19 13:15:19 - tokenizer_training_rank_0 - INFO - Training epochs: 30
2025-10-19 13:15:19 - tokenizer_training_rank_0 - INFO - Device: cuda:0
2025-10-19 13:15:19 - tokenizer_training_rank_0 - INFO - Distributed training: False
2025-10-19 13:15:19 - tokenizer_training_rank_0 - INFO - Starting tokenizer fine-tuning training...
2025-10-19 13:15:19 - tokenizer_training_rank_0 - INFO - Starting tokenizer training...
2025-10-19 13:16:50 - tokenizer_training_rank_0 - INFO - === Tokenizer Training Started ===
2025-10-19 13:16:50 - tokenizer_training_rank_0 - INFO - Experiment Name: BTCUSDT_1h_finetune
2025-10-19 13:16:50 - tokenizer_training_rank_0 - INFO - Log Directory: /root/project/Kronos-Btc-finetune-master/Kronos/finetune_csv/finetuned//BTCUSDT_1h_finetune/logs
2025-10-19 13:16:50 - tokenizer_training_rank_0 - INFO - Rank: 0
2025-10-19 13:16:50 - tokenizer_training_rank_0 - INFO - Timestamp: 2025-10-19 13:16:50
2025-10-19 13:16:50 - tokenizer_training_rank_0 - INFO - Loading pretrained tokenizer...
2025-10-19 13:16:50 - tokenizer_training_rank_0 - INFO - Tokenizer parameters: 3,958,042
2025-10-19 13:16:50 - tokenizer_training_rank_0 - INFO - === Training Configuration ===
2025-10-19 13:16:50 - tokenizer_training_rank_0 - INFO - Data path: /root/project/Kronos-Btc-finetune-master/Kronos/finetune_csv/data/BTCUSDT_1h_20251018_220012_fixed.csv
2025-10-19 13:16:50 - tokenizer_training_rank_0 - INFO - Lookback window: 512
2025-10-19 13:16:50 - tokenizer_training_rank_0 - INFO - Predict window: 48
2025-10-19 13:16:50 - tokenizer_training_rank_0 - INFO - Batch size: 32
2025-10-19 13:16:50 - tokenizer_training_rank_0 - INFO - Learning rate: 0.0002
2025-10-19 13:16:50 - tokenizer_training_rank_0 - INFO - Training epochs: 30
2025-10-19 13:16:50 - tokenizer_training_rank_0 - INFO - Device: cuda:0
2025-10-19 13:16:50 - tokenizer_training_rank_0 - INFO - Distributed training: False
2025-10-19 13:16:50 - tokenizer_training_rank_0 - INFO - Starting tokenizer fine-tuning training...
2025-10-19 13:16:50 - tokenizer_training_rank_0 - INFO - Starting tokenizer training...
2025-10-19 13:16:54 - tokenizer_training_rank_0 - INFO - [Epoch 1/30, Step 50/475] LR: 0.000026, Loss: -0.0294
2025-10-19 13:16:54 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0703
  - Recon Loss Pre: 0.0073
  - Recon Loss All: 0.0041
2025-10-19 13:16:58 - tokenizer_training_rank_0 - INFO - [Epoch 1/30, Step 100/475] LR: 0.000043, Loss: -0.0297
2025-10-19 13:16:58 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0704
  - Recon Loss Pre: 0.0071
  - Recon Loss All: 0.0039
2025-10-19 13:17:01 - tokenizer_training_rank_0 - INFO - [Epoch 1/30, Step 150/475] LR: 0.000070, Loss: -0.0299
2025-10-19 13:17:01 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0704
  - Recon Loss Pre: 0.0068
  - Recon Loss All: 0.0038
2025-10-19 13:17:04 - tokenizer_training_rank_0 - INFO - [Epoch 1/30, Step 200/475] LR: 0.000101, Loss: -0.0299
2025-10-19 13:17:04 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0704
  - Recon Loss Pre: 0.0069
  - Recon Loss All: 0.0037
2025-10-19 13:17:07 - tokenizer_training_rank_0 - INFO - [Epoch 1/30, Step 250/475] LR: 0.000134, Loss: -0.0300
2025-10-19 13:17:07 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0705
  - Recon Loss Pre: 0.0067
  - Recon Loss All: 0.0037
2025-10-19 13:17:11 - tokenizer_training_rank_0 - INFO - [Epoch 1/30, Step 300/475] LR: 0.000164, Loss: -0.0300
2025-10-19 13:17:11 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0705
  - Recon Loss Pre: 0.0070
  - Recon Loss All: 0.0036
2025-10-19 13:17:14 - tokenizer_training_rank_0 - INFO - [Epoch 1/30, Step 350/475] LR: 0.000186, Loss: -0.0303
2025-10-19 13:17:14 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0705
  - Recon Loss Pre: 0.0063
  - Recon Loss All: 0.0035
2025-10-19 13:17:17 - tokenizer_training_rank_0 - INFO - [Epoch 1/30, Step 400/475] LR: 0.000198, Loss: -0.0301
2025-10-19 13:17:17 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0706
  - Recon Loss Pre: 0.0065
  - Recon Loss All: 0.0038
2025-10-19 13:17:20 - tokenizer_training_rank_0 - INFO - [Epoch 1/30, Step 450/475] LR: 0.000200, Loss: -0.0306
2025-10-19 13:17:20 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0707
  - Recon Loss Pre: 0.0060
  - Recon Loss All: 0.0035
2025-10-19 13:17:23 - tokenizer_training_rank_0 - INFO - 
--- Epoch 1/30 Summary ---
Validation Loss: 0.0032
Epoch Time: 0:00:32
Total Training Time: 0:00:32

2025-10-19 13:17:23 - tokenizer_training_rank_0 - INFO - Best model saved to: /root/project/Kronos-Btc-finetune-master/Kronos/finetune_csv/finetuned//BTCUSDT_1h_finetune/tokenizer/best_model (validation loss: 0.0032)
2025-10-19 13:17:25 - tokenizer_training_rank_0 - INFO - [Epoch 2/30, Step 25/475] LR: 0.000200, Loss: -0.0309
2025-10-19 13:17:25 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0709
  - Recon Loss Pre: 0.0057
  - Recon Loss All: 0.0033
2025-10-19 13:17:28 - tokenizer_training_rank_0 - INFO - [Epoch 2/30, Step 75/475] LR: 0.000200, Loss: -0.0309
2025-10-19 13:17:28 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0709
  - Recon Loss Pre: 0.0057
  - Recon Loss All: 0.0033
2025-10-19 13:17:31 - tokenizer_training_rank_0 - INFO - [Epoch 2/30, Step 125/475] LR: 0.000200, Loss: -0.0303
2025-10-19 13:17:31 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0707
  - Recon Loss Pre: 0.0065
  - Recon Loss All: 0.0037
2025-10-19 13:17:35 - tokenizer_training_rank_0 - INFO - [Epoch 2/30, Step 175/475] LR: 0.000200, Loss: -0.0309
2025-10-19 13:17:35 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0708
  - Recon Loss Pre: 0.0057
  - Recon Loss All: 0.0033
2025-10-19 13:17:38 - tokenizer_training_rank_0 - INFO - [Epoch 2/30, Step 225/475] LR: 0.000200, Loss: -0.0308
2025-10-19 13:17:38 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0709
  - Recon Loss Pre: 0.0059
  - Recon Loss All: 0.0034
2025-10-19 13:17:41 - tokenizer_training_rank_0 - INFO - [Epoch 2/30, Step 275/475] LR: 0.000200, Loss: -0.0310
2025-10-19 13:17:41 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0709
  - Recon Loss Pre: 0.0057
  - Recon Loss All: 0.0031
2025-10-19 13:17:44 - tokenizer_training_rank_0 - INFO - [Epoch 2/30, Step 325/475] LR: 0.000200, Loss: -0.0310
2025-10-19 13:17:44 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0710
  - Recon Loss Pre: 0.0057
  - Recon Loss All: 0.0033
2025-10-19 13:17:47 - tokenizer_training_rank_0 - INFO - [Epoch 2/30, Step 375/475] LR: 0.000200, Loss: -0.0307
2025-10-19 13:17:47 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0709
  - Recon Loss Pre: 0.0060
  - Recon Loss All: 0.0034
2025-10-19 13:17:50 - tokenizer_training_rank_0 - INFO - [Epoch 2/30, Step 425/475] LR: 0.000199, Loss: -0.0309
2025-10-19 13:17:50 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0709
  - Recon Loss Pre: 0.0059
  - Recon Loss All: 0.0033
2025-10-19 13:17:54 - tokenizer_training_rank_0 - INFO - [Epoch 2/30, Step 475/475] LR: 0.000199, Loss: -0.0309
2025-10-19 13:17:54 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0710
  - Recon Loss Pre: 0.0059
  - Recon Loss All: 0.0033
2025-10-19 13:17:55 - tokenizer_training_rank_0 - INFO - 
--- Epoch 2/30 Summary ---
Validation Loss: 0.0032
Epoch Time: 0:00:31
Total Training Time: 0:00:31

2025-10-19 13:17:55 - tokenizer_training_rank_0 - INFO - Best model saved to: /root/project/Kronos-Btc-finetune-master/Kronos/finetune_csv/finetuned//BTCUSDT_1h_finetune/tokenizer/best_model (validation loss: 0.0032)
2025-10-19 13:17:58 - tokenizer_training_rank_0 - INFO - [Epoch 3/30, Step 50/475] LR: 0.000199, Loss: -0.0314
2025-10-19 13:17:58 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0711
  - Recon Loss Pre: 0.0053
  - Recon Loss All: 0.0030
2025-10-19 13:18:01 - tokenizer_training_rank_0 - INFO - [Epoch 3/30, Step 100/475] LR: 0.000199, Loss: -0.0311
2025-10-19 13:18:01 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0710
  - Recon Loss Pre: 0.0056
  - Recon Loss All: 0.0033
2025-10-19 13:18:04 - tokenizer_training_rank_0 - INFO - [Epoch 3/30, Step 150/475] LR: 0.000199, Loss: -0.0313
2025-10-19 13:18:04 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0711
  - Recon Loss Pre: 0.0055
  - Recon Loss All: 0.0030
2025-10-19 13:18:08 - tokenizer_training_rank_0 - INFO - [Epoch 3/30, Step 200/475] LR: 0.000199, Loss: -0.0309
2025-10-19 13:18:08 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0710
  - Recon Loss Pre: 0.0059
  - Recon Loss All: 0.0033
2025-10-19 13:18:11 - tokenizer_training_rank_0 - INFO - [Epoch 3/30, Step 250/475] LR: 0.000198, Loss: -0.0312
2025-10-19 13:18:11 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0711
  - Recon Loss Pre: 0.0055
  - Recon Loss All: 0.0031
2025-10-19 13:18:14 - tokenizer_training_rank_0 - INFO - [Epoch 3/30, Step 300/475] LR: 0.000198, Loss: -0.0314
2025-10-19 13:18:14 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0711
  - Recon Loss Pre: 0.0053
  - Recon Loss All: 0.0031
2025-10-19 13:18:17 - tokenizer_training_rank_0 - INFO - [Epoch 3/30, Step 350/475] LR: 0.000198, Loss: -0.0312
2025-10-19 13:18:17 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0711
  - Recon Loss Pre: 0.0055
  - Recon Loss All: 0.0032
2025-10-19 13:18:20 - tokenizer_training_rank_0 - INFO - [Epoch 3/30, Step 400/475] LR: 0.000198, Loss: -0.0318
2025-10-19 13:18:20 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0712
  - Recon Loss Pre: 0.0049
  - Recon Loss All: 0.0028
2025-10-19 13:18:24 - tokenizer_training_rank_0 - INFO - [Epoch 3/30, Step 450/475] LR: 0.000198, Loss: -0.0314
2025-10-19 13:18:24 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0711
  - Recon Loss Pre: 0.0052
  - Recon Loss All: 0.0030
2025-10-19 13:18:27 - tokenizer_training_rank_0 - INFO - 
--- Epoch 3/30 Summary ---
Validation Loss: 0.0032
Epoch Time: 0:00:31
Total Training Time: 0:00:31

2025-10-19 13:18:28 - tokenizer_training_rank_0 - INFO - [Epoch 4/30, Step 25/475] LR: 0.000197, Loss: -0.0313
2025-10-19 13:18:28 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0711
  - Recon Loss Pre: 0.0054
  - Recon Loss All: 0.0031
2025-10-19 13:18:31 - tokenizer_training_rank_0 - INFO - [Epoch 4/30, Step 75/475] LR: 0.000197, Loss: -0.0314
2025-10-19 13:18:31 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0711
  - Recon Loss Pre: 0.0052
  - Recon Loss All: 0.0030
2025-10-19 13:18:34 - tokenizer_training_rank_0 - INFO - [Epoch 4/30, Step 125/475] LR: 0.000197, Loss: -0.0311
2025-10-19 13:18:34 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0711
  - Recon Loss Pre: 0.0057
  - Recon Loss All: 0.0033
2025-10-19 13:18:38 - tokenizer_training_rank_0 - INFO - [Epoch 4/30, Step 175/475] LR: 0.000196, Loss: -0.0316
2025-10-19 13:18:38 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0712
  - Recon Loss Pre: 0.0052
  - Recon Loss All: 0.0029
2025-10-19 13:18:41 - tokenizer_training_rank_0 - INFO - [Epoch 4/30, Step 225/475] LR: 0.000196, Loss: -0.0312
2025-10-19 13:18:41 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0711
  - Recon Loss Pre: 0.0055
  - Recon Loss All: 0.0031
2025-10-19 13:18:44 - tokenizer_training_rank_0 - INFO - [Epoch 4/30, Step 275/475] LR: 0.000196, Loss: -0.0311
2025-10-19 13:18:44 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0711
  - Recon Loss Pre: 0.0057
  - Recon Loss All: 0.0032
2025-10-19 13:18:48 - tokenizer_training_rank_0 - INFO - [Epoch 4/30, Step 325/475] LR: 0.000196, Loss: -0.0312
2025-10-19 13:18:48 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0711
  - Recon Loss Pre: 0.0056
  - Recon Loss All: 0.0031
2025-10-19 13:18:51 - tokenizer_training_rank_0 - INFO - [Epoch 4/30, Step 375/475] LR: 0.000195, Loss: -0.0316
2025-10-19 13:18:51 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0711
  - Recon Loss Pre: 0.0050
  - Recon Loss All: 0.0029
2025-10-19 13:18:54 - tokenizer_training_rank_0 - INFO - [Epoch 4/30, Step 425/475] LR: 0.000195, Loss: -0.0312
2025-10-19 13:18:54 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0712
  - Recon Loss Pre: 0.0055
  - Recon Loss All: 0.0032
2025-10-19 13:18:57 - tokenizer_training_rank_0 - INFO - [Epoch 4/30, Step 475/475] LR: 0.000194, Loss: -0.0314
2025-10-19 13:18:57 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0711
  - Recon Loss Pre: 0.0055
  - Recon Loss All: 0.0030
2025-10-19 13:18:59 - tokenizer_training_rank_0 - INFO - 
--- Epoch 4/30 Summary ---
Validation Loss: 0.0031
Epoch Time: 0:00:32
Total Training Time: 0:00:32

2025-10-19 13:18:59 - tokenizer_training_rank_0 - INFO - Best model saved to: /root/project/Kronos-Btc-finetune-master/Kronos/finetune_csv/finetuned//BTCUSDT_1h_finetune/tokenizer/best_model (validation loss: 0.0031)
2025-10-19 13:19:02 - tokenizer_training_rank_0 - INFO - [Epoch 5/30, Step 50/475] LR: 0.000194, Loss: -0.0315
2025-10-19 13:19:02 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0712
  - Recon Loss Pre: 0.0053
  - Recon Loss All: 0.0030
2025-10-19 13:19:06 - tokenizer_training_rank_0 - INFO - [Epoch 5/30, Step 100/475] LR: 0.000194, Loss: -0.0316
2025-10-19 13:19:06 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0713
  - Recon Loss Pre: 0.0051
  - Recon Loss All: 0.0030
2025-10-19 13:19:09 - tokenizer_training_rank_0 - INFO - [Epoch 5/30, Step 150/475] LR: 0.000193, Loss: -0.0316
2025-10-19 13:19:09 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0713
  - Recon Loss Pre: 0.0052
  - Recon Loss All: 0.0030
2025-10-19 13:19:12 - tokenizer_training_rank_0 - INFO - [Epoch 5/30, Step 200/475] LR: 0.000193, Loss: -0.0318
2025-10-19 13:19:12 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0713
  - Recon Loss Pre: 0.0049
  - Recon Loss All: 0.0028
2025-10-19 13:19:15 - tokenizer_training_rank_0 - INFO - [Epoch 5/30, Step 250/475] LR: 0.000192, Loss: -0.0314
2025-10-19 13:19:15 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0713
  - Recon Loss Pre: 0.0053
  - Recon Loss All: 0.0031
2025-10-19 13:19:19 - tokenizer_training_rank_0 - INFO - [Epoch 5/30, Step 300/475] LR: 0.000192, Loss: -0.0316
2025-10-19 13:19:19 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0713
  - Recon Loss Pre: 0.0052
  - Recon Loss All: 0.0030
2025-10-19 13:19:22 - tokenizer_training_rank_0 - INFO - [Epoch 5/30, Step 350/475] LR: 0.000192, Loss: -0.0315
2025-10-19 13:19:22 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0712
  - Recon Loss Pre: 0.0052
  - Recon Loss All: 0.0031
2025-10-19 13:19:25 - tokenizer_training_rank_0 - INFO - [Epoch 5/30, Step 400/475] LR: 0.000191, Loss: -0.0317
2025-10-19 13:19:25 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0714
  - Recon Loss Pre: 0.0051
  - Recon Loss All: 0.0029
2025-10-19 13:19:28 - tokenizer_training_rank_0 - INFO - [Epoch 5/30, Step 450/475] LR: 0.000191, Loss: -0.0320
2025-10-19 13:19:28 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0714
  - Recon Loss Pre: 0.0047
  - Recon Loss All: 0.0027
2025-10-19 13:19:31 - tokenizer_training_rank_0 - INFO - 
--- Epoch 5/30 Summary ---
Validation Loss: 0.0030
Epoch Time: 0:00:32
Total Training Time: 0:00:32

2025-10-19 13:19:31 - tokenizer_training_rank_0 - INFO - Best model saved to: /root/project/Kronos-Btc-finetune-master/Kronos/finetune_csv/finetuned//BTCUSDT_1h_finetune/tokenizer/best_model (validation loss: 0.0030)
2025-10-19 13:19:33 - tokenizer_training_rank_0 - INFO - [Epoch 6/30, Step 25/475] LR: 0.000190, Loss: -0.0316
2025-10-19 13:19:33 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0713
  - Recon Loss Pre: 0.0053
  - Recon Loss All: 0.0029
2025-10-19 13:19:36 - tokenizer_training_rank_0 - INFO - [Epoch 6/30, Step 75/475] LR: 0.000190, Loss: -0.0316
2025-10-19 13:19:36 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0713
  - Recon Loss Pre: 0.0052
  - Recon Loss All: 0.0029
2025-10-19 13:19:39 - tokenizer_training_rank_0 - INFO - [Epoch 6/30, Step 125/475] LR: 0.000189, Loss: -0.0316
2025-10-19 13:19:39 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0713
  - Recon Loss Pre: 0.0052
  - Recon Loss All: 0.0029
2025-10-19 13:19:42 - tokenizer_training_rank_0 - INFO - [Epoch 6/30, Step 175/475] LR: 0.000189, Loss: -0.0314
2025-10-19 13:19:42 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0712
  - Recon Loss Pre: 0.0054
  - Recon Loss All: 0.0030
2025-10-19 13:19:46 - tokenizer_training_rank_0 - INFO - [Epoch 6/30, Step 225/475] LR: 0.000188, Loss: -0.0313
2025-10-19 13:19:46 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0711
  - Recon Loss Pre: 0.0055
  - Recon Loss All: 0.0031
2025-10-19 13:19:49 - tokenizer_training_rank_0 - INFO - [Epoch 6/30, Step 275/475] LR: 0.000188, Loss: -0.0315
2025-10-19 13:19:49 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0712
  - Recon Loss Pre: 0.0052
  - Recon Loss All: 0.0029
2025-10-19 13:19:52 - tokenizer_training_rank_0 - INFO - [Epoch 6/30, Step 325/475] LR: 0.000187, Loss: -0.0313
2025-10-19 13:19:52 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0712
  - Recon Loss Pre: 0.0055
  - Recon Loss All: 0.0031
2025-10-19 13:19:56 - tokenizer_training_rank_0 - INFO - [Epoch 6/30, Step 375/475] LR: 0.000186, Loss: -0.0315
2025-10-19 13:19:56 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0712
  - Recon Loss Pre: 0.0051
  - Recon Loss All: 0.0031
2025-10-19 13:19:59 - tokenizer_training_rank_0 - INFO - [Epoch 6/30, Step 425/475] LR: 0.000186, Loss: -0.0315
2025-10-19 13:19:59 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0712
  - Recon Loss Pre: 0.0052
  - Recon Loss All: 0.0030
2025-10-19 13:20:02 - tokenizer_training_rank_0 - INFO - [Epoch 6/30, Step 475/475] LR: 0.000185, Loss: -0.0317
2025-10-19 13:20:02 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0713
  - Recon Loss Pre: 0.0050
  - Recon Loss All: 0.0028
2025-10-19 13:20:03 - tokenizer_training_rank_0 - INFO - 
--- Epoch 6/30 Summary ---
Validation Loss: 0.0031
Epoch Time: 0:00:32
Total Training Time: 0:00:32

2025-10-19 13:20:07 - tokenizer_training_rank_0 - INFO - [Epoch 7/30, Step 50/475] LR: 0.000185, Loss: -0.0316
2025-10-19 13:20:07 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0712
  - Recon Loss Pre: 0.0051
  - Recon Loss All: 0.0029
2025-10-19 13:20:10 - tokenizer_training_rank_0 - INFO - [Epoch 7/30, Step 100/475] LR: 0.000184, Loss: -0.0316
2025-10-19 13:20:10 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0712
  - Recon Loss Pre: 0.0052
  - Recon Loss All: 0.0029
2025-10-19 13:20:13 - tokenizer_training_rank_0 - INFO - [Epoch 7/30, Step 150/475] LR: 0.000183, Loss: -0.0317
2025-10-19 13:20:13 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0713
  - Recon Loss Pre: 0.0049
  - Recon Loss All: 0.0029
2025-10-19 13:20:17 - tokenizer_training_rank_0 - INFO - [Epoch 7/30, Step 200/475] LR: 0.000183, Loss: -0.0314
2025-10-19 13:20:17 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0713
  - Recon Loss Pre: 0.0055
  - Recon Loss All: 0.0030
2025-10-19 13:20:20 - tokenizer_training_rank_0 - INFO - [Epoch 7/30, Step 250/475] LR: 0.000182, Loss: -0.0315
2025-10-19 13:20:20 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0712
  - Recon Loss Pre: 0.0052
  - Recon Loss All: 0.0030
2025-10-19 13:20:24 - tokenizer_training_rank_0 - INFO - [Epoch 7/30, Step 300/475] LR: 0.000181, Loss: -0.0315
2025-10-19 13:20:24 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0712
  - Recon Loss Pre: 0.0052
  - Recon Loss All: 0.0030
2025-10-19 13:20:27 - tokenizer_training_rank_0 - INFO - [Epoch 7/30, Step 350/475] LR: 0.000181, Loss: -0.0316
2025-10-19 13:20:27 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0712
  - Recon Loss Pre: 0.0051
  - Recon Loss All: 0.0030
2025-10-19 13:20:30 - tokenizer_training_rank_0 - INFO - [Epoch 7/30, Step 400/475] LR: 0.000180, Loss: -0.0314
2025-10-19 13:20:30 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0712
  - Recon Loss Pre: 0.0052
  - Recon Loss All: 0.0032
2025-10-19 13:20:34 - tokenizer_training_rank_0 - INFO - [Epoch 7/30, Step 450/475] LR: 0.000179, Loss: -0.0319
2025-10-19 13:20:34 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0714
  - Recon Loss Pre: 0.0048
  - Recon Loss All: 0.0028
2025-10-19 13:20:36 - tokenizer_training_rank_0 - INFO - 
--- Epoch 7/30 Summary ---
Validation Loss: 0.0031
Epoch Time: 0:00:32
Total Training Time: 0:00:32

2025-10-19 13:20:38 - tokenizer_training_rank_0 - INFO - [Epoch 8/30, Step 25/475] LR: 0.000179, Loss: -0.0314
2025-10-19 13:20:38 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0713
  - Recon Loss Pre: 0.0054
  - Recon Loss All: 0.0031
2025-10-19 13:20:41 - tokenizer_training_rank_0 - INFO - [Epoch 8/30, Step 75/475] LR: 0.000178, Loss: -0.0317
2025-10-19 13:20:41 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0713
  - Recon Loss Pre: 0.0049
  - Recon Loss All: 0.0029
2025-10-19 13:20:44 - tokenizer_training_rank_0 - INFO - [Epoch 8/30, Step 125/475] LR: 0.000177, Loss: -0.0315
2025-10-19 13:20:44 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0713
  - Recon Loss Pre: 0.0053
  - Recon Loss All: 0.0031
2025-10-19 13:20:48 - tokenizer_training_rank_0 - INFO - [Epoch 8/30, Step 175/475] LR: 0.000177, Loss: -0.0319
2025-10-19 13:20:48 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0713
  - Recon Loss Pre: 0.0047
  - Recon Loss All: 0.0027
2025-10-19 13:20:51 - tokenizer_training_rank_0 - INFO - [Epoch 8/30, Step 225/475] LR: 0.000176, Loss: -0.0315
2025-10-19 13:20:51 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0713
  - Recon Loss Pre: 0.0052
  - Recon Loss All: 0.0030
2025-10-19 13:20:54 - tokenizer_training_rank_0 - INFO - [Epoch 8/30, Step 275/475] LR: 0.000175, Loss: -0.0316
2025-10-19 13:20:54 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0713
  - Recon Loss Pre: 0.0051
  - Recon Loss All: 0.0029
2025-10-19 13:20:58 - tokenizer_training_rank_0 - INFO - [Epoch 8/30, Step 325/475] LR: 0.000174, Loss: -0.0314
2025-10-19 13:20:58 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0712
  - Recon Loss Pre: 0.0054
  - Recon Loss All: 0.0031
2025-10-19 13:21:01 - tokenizer_training_rank_0 - INFO - [Epoch 8/30, Step 375/475] LR: 0.000174, Loss: -0.0317
2025-10-19 13:21:01 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0713
  - Recon Loss Pre: 0.0049
  - Recon Loss All: 0.0030
2025-10-19 13:21:05 - tokenizer_training_rank_0 - INFO - [Epoch 8/30, Step 425/475] LR: 0.000173, Loss: -0.0316
2025-10-19 13:21:05 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0713
  - Recon Loss Pre: 0.0052
  - Recon Loss All: 0.0030
2025-10-19 13:21:08 - tokenizer_training_rank_0 - INFO - [Epoch 8/30, Step 475/475] LR: 0.000172, Loss: -0.0319
2025-10-19 13:21:08 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0713
  - Recon Loss Pre: 0.0047
  - Recon Loss All: 0.0028
2025-10-19 13:21:09 - tokenizer_training_rank_0 - INFO - 
--- Epoch 8/30 Summary ---
Validation Loss: 0.0031
Epoch Time: 0:00:32
Total Training Time: 0:00:32

2025-10-19 13:21:12 - tokenizer_training_rank_0 - INFO - [Epoch 9/30, Step 50/475] LR: 0.000171, Loss: -0.0315
2025-10-19 13:21:12 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0713
  - Recon Loss Pre: 0.0052
  - Recon Loss All: 0.0030
2025-10-19 13:21:16 - tokenizer_training_rank_0 - INFO - [Epoch 9/30, Step 100/475] LR: 0.000170, Loss: -0.0314
2025-10-19 13:21:16 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0712
  - Recon Loss Pre: 0.0052
  - Recon Loss All: 0.0032
2025-10-19 13:21:19 - tokenizer_training_rank_0 - INFO - [Epoch 9/30, Step 150/475] LR: 0.000170, Loss: -0.0318
2025-10-19 13:21:19 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0713
  - Recon Loss Pre: 0.0048
  - Recon Loss All: 0.0029
2025-10-19 13:21:22 - tokenizer_training_rank_0 - INFO - [Epoch 9/30, Step 200/475] LR: 0.000169, Loss: -0.0314
2025-10-19 13:21:22 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0713
  - Recon Loss Pre: 0.0055
  - Recon Loss All: 0.0030
2025-10-19 13:21:26 - tokenizer_training_rank_0 - INFO - [Epoch 9/30, Step 250/475] LR: 0.000168, Loss: -0.0317
2025-10-19 13:21:26 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0713
  - Recon Loss Pre: 0.0050
  - Recon Loss All: 0.0029
2025-10-19 13:21:29 - tokenizer_training_rank_0 - INFO - [Epoch 9/30, Step 300/475] LR: 0.000167, Loss: -0.0316
2025-10-19 13:21:29 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0712
  - Recon Loss Pre: 0.0051
  - Recon Loss All: 0.0030
2025-10-19 13:21:32 - tokenizer_training_rank_0 - INFO - [Epoch 9/30, Step 350/475] LR: 0.000166, Loss: -0.0314
2025-10-19 13:21:32 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0712
  - Recon Loss Pre: 0.0053
  - Recon Loss All: 0.0031
2025-10-19 13:21:35 - tokenizer_training_rank_0 - INFO - [Epoch 9/30, Step 400/475] LR: 0.000165, Loss: -0.0317
2025-10-19 13:21:35 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0713
  - Recon Loss Pre: 0.0049
  - Recon Loss All: 0.0029
2025-10-19 13:21:39 - tokenizer_training_rank_0 - INFO - [Epoch 9/30, Step 450/475] LR: 0.000165, Loss: -0.0316
2025-10-19 13:21:39 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0713
  - Recon Loss Pre: 0.0051
  - Recon Loss All: 0.0030
2025-10-19 13:21:41 - tokenizer_training_rank_0 - INFO - 
--- Epoch 9/30 Summary ---
Validation Loss: 0.0031
Epoch Time: 0:00:32
Total Training Time: 0:00:32

2025-10-19 13:21:43 - tokenizer_training_rank_0 - INFO - [Epoch 10/30, Step 25/475] LR: 0.000164, Loss: -0.0315
2025-10-19 13:21:43 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0713
  - Recon Loss Pre: 0.0052
  - Recon Loss All: 0.0031
2025-10-19 13:21:46 - tokenizer_training_rank_0 - INFO - [Epoch 10/30, Step 75/475] LR: 0.000163, Loss: -0.0316
2025-10-19 13:21:46 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0712
  - Recon Loss Pre: 0.0051
  - Recon Loss All: 0.0029
2025-10-19 13:21:50 - tokenizer_training_rank_0 - INFO - [Epoch 10/30, Step 125/475] LR: 0.000162, Loss: -0.0315
2025-10-19 13:21:50 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0713
  - Recon Loss Pre: 0.0052
  - Recon Loss All: 0.0031
2025-10-19 13:21:53 - tokenizer_training_rank_0 - INFO - [Epoch 10/30, Step 175/475] LR: 0.000161, Loss: -0.0318
2025-10-19 13:21:53 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0713
  - Recon Loss Pre: 0.0048
  - Recon Loss All: 0.0028
2025-10-19 13:21:56 - tokenizer_training_rank_0 - INFO - [Epoch 10/30, Step 225/475] LR: 0.000160, Loss: -0.0316
2025-10-19 13:21:56 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0713
  - Recon Loss Pre: 0.0052
  - Recon Loss All: 0.0030
2025-10-19 13:22:00 - tokenizer_training_rank_0 - INFO - [Epoch 10/30, Step 275/475] LR: 0.000159, Loss: -0.0319
2025-10-19 13:22:00 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0714
  - Recon Loss Pre: 0.0048
  - Recon Loss All: 0.0028
2025-10-19 13:22:03 - tokenizer_training_rank_0 - INFO - [Epoch 10/30, Step 325/475] LR: 0.000158, Loss: -0.0319
2025-10-19 13:22:03 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0714
  - Recon Loss Pre: 0.0047
  - Recon Loss All: 0.0028
2025-10-19 13:22:07 - tokenizer_training_rank_0 - INFO - [Epoch 10/30, Step 375/475] LR: 0.000157, Loss: -0.0318
2025-10-19 13:22:07 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0714
  - Recon Loss Pre: 0.0049
  - Recon Loss All: 0.0029
2025-10-19 13:22:10 - tokenizer_training_rank_0 - INFO - [Epoch 10/30, Step 425/475] LR: 0.000156, Loss: -0.0318
2025-10-19 13:22:10 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0714
  - Recon Loss Pre: 0.0048
  - Recon Loss All: 0.0029
2025-10-19 13:22:13 - tokenizer_training_rank_0 - INFO - [Epoch 10/30, Step 475/475] LR: 0.000155, Loss: -0.0320
2025-10-19 13:22:13 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0714
  - Recon Loss Pre: 0.0046
  - Recon Loss All: 0.0028
2025-10-19 13:22:14 - tokenizer_training_rank_0 - INFO - 
--- Epoch 10/30 Summary ---
Validation Loss: 0.0031
Epoch Time: 0:00:32
Total Training Time: 0:00:32

2025-10-19 13:22:17 - tokenizer_training_rank_0 - INFO - [Epoch 11/30, Step 50/475] LR: 0.000155, Loss: -0.0315
2025-10-19 13:22:17 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0713
  - Recon Loss Pre: 0.0052
  - Recon Loss All: 0.0031
2025-10-19 13:22:21 - tokenizer_training_rank_0 - INFO - [Epoch 11/30, Step 100/475] LR: 0.000154, Loss: -0.0320
2025-10-19 13:22:21 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0714
  - Recon Loss Pre: 0.0046
  - Recon Loss All: 0.0027
2025-10-19 13:22:24 - tokenizer_training_rank_0 - INFO - [Epoch 11/30, Step 150/475] LR: 0.000153, Loss: -0.0316
2025-10-19 13:22:24 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0713
  - Recon Loss Pre: 0.0051
  - Recon Loss All: 0.0031
2025-10-19 13:22:27 - tokenizer_training_rank_0 - INFO - [Epoch 11/30, Step 200/475] LR: 0.000152, Loss: -0.0318
2025-10-19 13:22:27 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0714
  - Recon Loss Pre: 0.0049
  - Recon Loss All: 0.0029
2025-10-19 13:22:31 - tokenizer_training_rank_0 - INFO - [Epoch 11/30, Step 250/475] LR: 0.000151, Loss: -0.0320
2025-10-19 13:22:31 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0714
  - Recon Loss Pre: 0.0046
  - Recon Loss All: 0.0028
2025-10-19 13:22:34 - tokenizer_training_rank_0 - INFO - [Epoch 11/30, Step 300/475] LR: 0.000150, Loss: -0.0319
2025-10-19 13:22:34 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0714
  - Recon Loss Pre: 0.0047
  - Recon Loss All: 0.0029
2025-10-19 13:22:37 - tokenizer_training_rank_0 - INFO - [Epoch 11/30, Step 350/475] LR: 0.000149, Loss: -0.0319
2025-10-19 13:22:37 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0714
  - Recon Loss Pre: 0.0047
  - Recon Loss All: 0.0029
2025-10-19 13:22:41 - tokenizer_training_rank_0 - INFO - [Epoch 11/30, Step 400/475] LR: 0.000148, Loss: -0.0320
2025-10-19 13:22:41 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0715
  - Recon Loss Pre: 0.0046
  - Recon Loss All: 0.0029
2025-10-19 13:22:44 - tokenizer_training_rank_0 - INFO - [Epoch 11/30, Step 450/475] LR: 0.000147, Loss: -0.0319
2025-10-19 13:22:44 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0714
  - Recon Loss Pre: 0.0047
  - Recon Loss All: 0.0029
2025-10-19 13:22:47 - tokenizer_training_rank_0 - INFO - 
--- Epoch 11/30 Summary ---
Validation Loss: 0.0031
Epoch Time: 0:00:32
Total Training Time: 0:00:32

2025-10-19 13:22:48 - tokenizer_training_rank_0 - INFO - [Epoch 12/30, Step 25/475] LR: 0.000146, Loss: -0.0321
2025-10-19 13:22:48 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0714
  - Recon Loss Pre: 0.0045
  - Recon Loss All: 0.0027
2025-10-19 13:22:51 - tokenizer_training_rank_0 - INFO - [Epoch 12/30, Step 75/475] LR: 0.000145, Loss: -0.0321
2025-10-19 13:22:51 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0714
  - Recon Loss Pre: 0.0045
  - Recon Loss All: 0.0028
2025-10-19 13:22:55 - tokenizer_training_rank_0 - INFO - [Epoch 12/30, Step 125/475] LR: 0.000144, Loss: -0.0317
2025-10-19 13:22:55 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0714
  - Recon Loss Pre: 0.0050
  - Recon Loss All: 0.0030
2025-10-19 13:22:58 - tokenizer_training_rank_0 - INFO - [Epoch 12/30, Step 175/475] LR: 0.000143, Loss: -0.0320
2025-10-19 13:22:58 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0715
  - Recon Loss Pre: 0.0045
  - Recon Loss All: 0.0028
2025-10-19 13:23:02 - tokenizer_training_rank_0 - INFO - [Epoch 12/30, Step 225/475] LR: 0.000142, Loss: -0.0320
2025-10-19 13:23:02 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0715
  - Recon Loss Pre: 0.0047
  - Recon Loss All: 0.0028
2025-10-19 13:23:05 - tokenizer_training_rank_0 - INFO - [Epoch 12/30, Step 275/475] LR: 0.000141, Loss: -0.0319
2025-10-19 13:23:05 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0715
  - Recon Loss Pre: 0.0047
  - Recon Loss All: 0.0030
2025-10-19 13:23:08 - tokenizer_training_rank_0 - INFO - [Epoch 12/30, Step 325/475] LR: 0.000140, Loss: -0.0318
2025-10-19 13:23:08 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0714
  - Recon Loss Pre: 0.0049
  - Recon Loss All: 0.0030
2025-10-19 13:23:11 - tokenizer_training_rank_0 - INFO - [Epoch 12/30, Step 375/475] LR: 0.000138, Loss: -0.0320
2025-10-19 13:23:11 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0715
  - Recon Loss Pre: 0.0046
  - Recon Loss All: 0.0029
2025-10-19 13:23:15 - tokenizer_training_rank_0 - INFO - [Epoch 12/30, Step 425/475] LR: 0.000137, Loss: -0.0321
2025-10-19 13:23:15 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0715
  - Recon Loss Pre: 0.0045
  - Recon Loss All: 0.0027
2025-10-19 13:23:18 - tokenizer_training_rank_0 - INFO - [Epoch 12/30, Step 475/475] LR: 0.000136, Loss: -0.0320
2025-10-19 13:23:18 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0715
  - Recon Loss Pre: 0.0047
  - Recon Loss All: 0.0028
2025-10-19 13:23:19 - tokenizer_training_rank_0 - INFO - 
--- Epoch 12/30 Summary ---
Validation Loss: 0.0030
Epoch Time: 0:00:32
Total Training Time: 0:00:32

2025-10-19 13:23:22 - tokenizer_training_rank_0 - INFO - [Epoch 13/30, Step 50/475] LR: 0.000135, Loss: -0.0320
2025-10-19 13:23:22 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0715
  - Recon Loss Pre: 0.0047
  - Recon Loss All: 0.0028
2025-10-19 13:23:25 - tokenizer_training_rank_0 - INFO - [Epoch 13/30, Step 100/475] LR: 0.000134, Loss: -0.0322
2025-10-19 13:23:25 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0715
  - Recon Loss Pre: 0.0044
  - Recon Loss All: 0.0027
2025-10-19 13:23:28 - tokenizer_training_rank_0 - INFO - [Epoch 13/30, Step 150/475] LR: 0.000133, Loss: -0.0321
2025-10-19 13:23:28 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0715
  - Recon Loss Pre: 0.0045
  - Recon Loss All: 0.0028
2025-10-19 13:23:31 - tokenizer_training_rank_0 - INFO - [Epoch 13/30, Step 200/475] LR: 0.000132, Loss: -0.0314
2025-10-19 13:23:31 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0715
  - Recon Loss Pre: 0.0059
  - Recon Loss All: 0.0029
2025-10-19 13:23:35 - tokenizer_training_rank_0 - INFO - [Epoch 13/30, Step 250/475] LR: 0.000131, Loss: -0.0323
2025-10-19 13:23:35 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0715
  - Recon Loss Pre: 0.0044
  - Recon Loss All: 0.0026
2025-10-19 13:23:38 - tokenizer_training_rank_0 - INFO - [Epoch 13/30, Step 300/475] LR: 0.000130, Loss: -0.0323
2025-10-19 13:23:38 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0715
  - Recon Loss Pre: 0.0043
  - Recon Loss All: 0.0026
2025-10-19 13:23:41 - tokenizer_training_rank_0 - INFO - [Epoch 13/30, Step 350/475] LR: 0.000129, Loss: -0.0320
2025-10-19 13:23:41 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0715
  - Recon Loss Pre: 0.0047
  - Recon Loss All: 0.0028
2025-10-19 13:23:44 - tokenizer_training_rank_0 - INFO - [Epoch 13/30, Step 400/475] LR: 0.000128, Loss: -0.0323
2025-10-19 13:23:44 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0043
  - Recon Loss All: 0.0026
2025-10-19 13:23:48 - tokenizer_training_rank_0 - INFO - [Epoch 13/30, Step 450/475] LR: 0.000127, Loss: -0.0320
2025-10-19 13:23:48 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0715
  - Recon Loss Pre: 0.0047
  - Recon Loss All: 0.0029
2025-10-19 13:23:50 - tokenizer_training_rank_0 - INFO - 
--- Epoch 13/30 Summary ---
Validation Loss: 0.0030
Epoch Time: 0:00:31
Total Training Time: 0:00:31

2025-10-19 13:23:50 - tokenizer_training_rank_0 - INFO - Best model saved to: /root/project/Kronos-Btc-finetune-master/Kronos/finetune_csv/finetuned//BTCUSDT_1h_finetune/tokenizer/best_model (validation loss: 0.0030)
2025-10-19 13:23:52 - tokenizer_training_rank_0 - INFO - [Epoch 14/30, Step 25/475] LR: 0.000126, Loss: -0.0320
2025-10-19 13:23:52 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0715
  - Recon Loss Pre: 0.0047
  - Recon Loss All: 0.0028
2025-10-19 13:23:55 - tokenizer_training_rank_0 - INFO - [Epoch 14/30, Step 75/475] LR: 0.000124, Loss: -0.0322
2025-10-19 13:23:55 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0715
  - Recon Loss Pre: 0.0044
  - Recon Loss All: 0.0028
2025-10-19 13:23:59 - tokenizer_training_rank_0 - INFO - [Epoch 14/30, Step 125/475] LR: 0.000123, Loss: -0.0321
2025-10-19 13:23:59 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0715
  - Recon Loss Pre: 0.0046
  - Recon Loss All: 0.0027
2025-10-19 13:24:02 - tokenizer_training_rank_0 - INFO - [Epoch 14/30, Step 175/475] LR: 0.000122, Loss: -0.0321
2025-10-19 13:24:02 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0047
  - Recon Loss All: 0.0028
2025-10-19 13:24:06 - tokenizer_training_rank_0 - INFO - [Epoch 14/30, Step 225/475] LR: 0.000121, Loss: -0.0321
2025-10-19 13:24:06 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0046
  - Recon Loss All: 0.0027
2025-10-19 13:24:09 - tokenizer_training_rank_0 - INFO - [Epoch 14/30, Step 275/475] LR: 0.000120, Loss: -0.0321
2025-10-19 13:24:09 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0046
  - Recon Loss All: 0.0028
2025-10-19 13:24:12 - tokenizer_training_rank_0 - INFO - [Epoch 14/30, Step 325/475] LR: 0.000119, Loss: -0.0322
2025-10-19 13:24:12 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0046
  - Recon Loss All: 0.0027
2025-10-19 13:24:15 - tokenizer_training_rank_0 - INFO - [Epoch 14/30, Step 375/475] LR: 0.000118, Loss: -0.0323
2025-10-19 13:24:15 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0043
  - Recon Loss All: 0.0027
2025-10-19 13:24:19 - tokenizer_training_rank_0 - INFO - [Epoch 14/30, Step 425/475] LR: 0.000117, Loss: -0.0324
2025-10-19 13:24:19 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0042
  - Recon Loss All: 0.0027
2025-10-19 13:24:22 - tokenizer_training_rank_0 - INFO - [Epoch 14/30, Step 475/475] LR: 0.000116, Loss: -0.0324
2025-10-19 13:24:22 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0043
  - Recon Loss All: 0.0025
2025-10-19 13:24:23 - tokenizer_training_rank_0 - INFO - 
--- Epoch 14/30 Summary ---
Validation Loss: 0.0030
Epoch Time: 0:00:32
Total Training Time: 0:00:32

2025-10-19 13:24:23 - tokenizer_training_rank_0 - INFO - Best model saved to: /root/project/Kronos-Btc-finetune-master/Kronos/finetune_csv/finetuned//BTCUSDT_1h_finetune/tokenizer/best_model (validation loss: 0.0030)
2025-10-19 13:24:26 - tokenizer_training_rank_0 - INFO - [Epoch 15/30, Step 50/475] LR: 0.000114, Loss: -0.0320
2025-10-19 13:24:26 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0048
  - Recon Loss All: 0.0029
2025-10-19 13:24:30 - tokenizer_training_rank_0 - INFO - [Epoch 15/30, Step 100/475] LR: 0.000113, Loss: -0.0322
2025-10-19 13:24:30 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0046
  - Recon Loss All: 0.0026
2025-10-19 13:24:33 - tokenizer_training_rank_0 - INFO - [Epoch 15/30, Step 150/475] LR: 0.000112, Loss: -0.0323
2025-10-19 13:24:33 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0044
  - Recon Loss All: 0.0026
2025-10-19 13:24:36 - tokenizer_training_rank_0 - INFO - [Epoch 15/30, Step 200/475] LR: 0.000111, Loss: -0.0323
2025-10-19 13:24:36 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0043
  - Recon Loss All: 0.0026
2025-10-19 13:24:40 - tokenizer_training_rank_0 - INFO - [Epoch 15/30, Step 250/475] LR: 0.000110, Loss: -0.0318
2025-10-19 13:24:40 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0715
  - Recon Loss Pre: 0.0050
  - Recon Loss All: 0.0029
2025-10-19 13:24:43 - tokenizer_training_rank_0 - INFO - [Epoch 15/30, Step 300/475] LR: 0.000109, Loss: -0.0324
2025-10-19 13:24:43 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0042
  - Recon Loss All: 0.0027
2025-10-19 13:24:46 - tokenizer_training_rank_0 - INFO - [Epoch 15/30, Step 350/475] LR: 0.000108, Loss: -0.0323
2025-10-19 13:24:46 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0044
  - Recon Loss All: 0.0027
2025-10-19 13:24:50 - tokenizer_training_rank_0 - INFO - [Epoch 15/30, Step 400/475] LR: 0.000107, Loss: -0.0322
2025-10-19 13:24:50 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0045
  - Recon Loss All: 0.0028
2025-10-19 13:24:53 - tokenizer_training_rank_0 - INFO - [Epoch 15/30, Step 450/475] LR: 0.000105, Loss: -0.0322
2025-10-19 13:24:53 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0046
  - Recon Loss All: 0.0026
2025-10-19 13:24:56 - tokenizer_training_rank_0 - INFO - 
--- Epoch 15/30 Summary ---
Validation Loss: 0.0030
Epoch Time: 0:00:32
Total Training Time: 0:00:32

2025-10-19 13:24:58 - tokenizer_training_rank_0 - INFO - [Epoch 16/30, Step 25/475] LR: 0.000104, Loss: -0.0321
2025-10-19 13:24:58 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0046
  - Recon Loss All: 0.0027
2025-10-19 13:25:01 - tokenizer_training_rank_0 - INFO - [Epoch 16/30, Step 75/475] LR: 0.000103, Loss: -0.0322
2025-10-19 13:25:01 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0045
  - Recon Loss All: 0.0027
2025-10-19 13:25:04 - tokenizer_training_rank_0 - INFO - [Epoch 16/30, Step 125/475] LR: 0.000102, Loss: -0.0320
2025-10-19 13:25:04 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0048
  - Recon Loss All: 0.0028
2025-10-19 13:25:08 - tokenizer_training_rank_0 - INFO - [Epoch 16/30, Step 175/475] LR: 0.000101, Loss: -0.0320
2025-10-19 13:25:08 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0047
  - Recon Loss All: 0.0029
2025-10-19 13:25:11 - tokenizer_training_rank_0 - INFO - [Epoch 16/30, Step 225/475] LR: 0.000100, Loss: -0.0319
2025-10-19 13:25:11 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0715
  - Recon Loss Pre: 0.0048
  - Recon Loss All: 0.0029
2025-10-19 13:25:14 - tokenizer_training_rank_0 - INFO - [Epoch 16/30, Step 275/475] LR: 0.000099, Loss: -0.0322
2025-10-19 13:25:14 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0045
  - Recon Loss All: 0.0027
2025-10-19 13:25:18 - tokenizer_training_rank_0 - INFO - [Epoch 16/30, Step 325/475] LR: 0.000097, Loss: -0.0323
2025-10-19 13:25:18 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0044
  - Recon Loss All: 0.0027
2025-10-19 13:25:21 - tokenizer_training_rank_0 - INFO - [Epoch 16/30, Step 375/475] LR: 0.000096, Loss: -0.0318
2025-10-19 13:25:21 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0050
  - Recon Loss All: 0.0029
2025-10-19 13:25:24 - tokenizer_training_rank_0 - INFO - [Epoch 16/30, Step 425/475] LR: 0.000095, Loss: -0.0322
2025-10-19 13:25:24 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0044
  - Recon Loss All: 0.0028
2025-10-19 13:25:27 - tokenizer_training_rank_0 - INFO - [Epoch 16/30, Step 475/475] LR: 0.000094, Loss: -0.0322
2025-10-19 13:25:27 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0046
  - Recon Loss All: 0.0026
2025-10-19 13:25:29 - tokenizer_training_rank_0 - INFO - 
--- Epoch 16/30 Summary ---
Validation Loss: 0.0031
Epoch Time: 0:00:32
Total Training Time: 0:00:32

2025-10-19 13:25:32 - tokenizer_training_rank_0 - INFO - [Epoch 17/30, Step 50/475] LR: 0.000093, Loss: -0.0319
2025-10-19 13:25:32 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0048
  - Recon Loss All: 0.0030
2025-10-19 13:25:35 - tokenizer_training_rank_0 - INFO - [Epoch 17/30, Step 100/475] LR: 0.000092, Loss: -0.0319
2025-10-19 13:25:35 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0050
  - Recon Loss All: 0.0027
2025-10-19 13:25:39 - tokenizer_training_rank_0 - INFO - [Epoch 17/30, Step 150/475] LR: 0.000091, Loss: -0.0321
2025-10-19 13:25:39 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0046
  - Recon Loss All: 0.0027
2025-10-19 13:25:42 - tokenizer_training_rank_0 - INFO - [Epoch 17/30, Step 200/475] LR: 0.000090, Loss: -0.0322
2025-10-19 13:25:42 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0044
  - Recon Loss All: 0.0028
2025-10-19 13:25:45 - tokenizer_training_rank_0 - INFO - [Epoch 17/30, Step 250/475] LR: 0.000088, Loss: -0.0320
2025-10-19 13:25:45 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0047
  - Recon Loss All: 0.0028
2025-10-19 13:25:48 - tokenizer_training_rank_0 - INFO - [Epoch 17/30, Step 300/475] LR: 0.000087, Loss: -0.0321
2025-10-19 13:25:48 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0046
  - Recon Loss All: 0.0027
2025-10-19 13:25:52 - tokenizer_training_rank_0 - INFO - [Epoch 17/30, Step 350/475] LR: 0.000086, Loss: -0.0321
2025-10-19 13:25:52 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0046
  - Recon Loss All: 0.0027
2025-10-19 13:25:55 - tokenizer_training_rank_0 - INFO - [Epoch 17/30, Step 400/475] LR: 0.000085, Loss: -0.0321
2025-10-19 13:25:55 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0047
  - Recon Loss All: 0.0027
2025-10-19 13:25:59 - tokenizer_training_rank_0 - INFO - [Epoch 17/30, Step 450/475] LR: 0.000084, Loss: -0.0323
2025-10-19 13:25:59 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0044
  - Recon Loss All: 0.0027
2025-10-19 13:26:01 - tokenizer_training_rank_0 - INFO - 
--- Epoch 17/30 Summary ---
Validation Loss: 0.0031
Epoch Time: 0:00:32
Total Training Time: 0:00:32

2025-10-19 13:26:03 - tokenizer_training_rank_0 - INFO - [Epoch 18/30, Step 25/475] LR: 0.000083, Loss: -0.0321
2025-10-19 13:26:03 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0046
  - Recon Loss All: 0.0028
2025-10-19 13:26:06 - tokenizer_training_rank_0 - INFO - [Epoch 18/30, Step 75/475] LR: 0.000082, Loss: -0.0322
2025-10-19 13:26:06 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0045
  - Recon Loss All: 0.0027
2025-10-19 13:26:09 - tokenizer_training_rank_0 - INFO - [Epoch 18/30, Step 125/475] LR: 0.000081, Loss: -0.0320
2025-10-19 13:26:09 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0047
  - Recon Loss All: 0.0028
2025-10-19 13:26:12 - tokenizer_training_rank_0 - INFO - [Epoch 18/30, Step 175/475] LR: 0.000079, Loss: -0.0322
2025-10-19 13:26:12 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0045
  - Recon Loss All: 0.0027
2025-10-19 13:26:15 - tokenizer_training_rank_0 - INFO - [Epoch 18/30, Step 225/475] LR: 0.000078, Loss: -0.0320
2025-10-19 13:26:15 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0047
  - Recon Loss All: 0.0028
2025-10-19 13:26:18 - tokenizer_training_rank_0 - INFO - [Epoch 18/30, Step 275/475] LR: 0.000077, Loss: -0.0322
2025-10-19 13:26:18 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0044
  - Recon Loss All: 0.0028
2025-10-19 13:26:20 - tokenizer_training_rank_0 - INFO - [Epoch 18/30, Step 325/475] LR: 0.000076, Loss: -0.0323
2025-10-19 13:26:21 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0044
  - Recon Loss All: 0.0027
2025-10-19 13:26:23 - tokenizer_training_rank_0 - INFO - [Epoch 18/30, Step 375/475] LR: 0.000075, Loss: -0.0323
2025-10-19 13:26:23 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0043
  - Recon Loss All: 0.0026
2025-10-19 13:26:26 - tokenizer_training_rank_0 - INFO - [Epoch 18/30, Step 425/475] LR: 0.000074, Loss: -0.0323
2025-10-19 13:26:26 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0044
  - Recon Loss All: 0.0027
2025-10-19 13:26:29 - tokenizer_training_rank_0 - INFO - [Epoch 18/30, Step 475/475] LR: 0.000073, Loss: -0.0323
2025-10-19 13:26:29 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0043
  - Recon Loss All: 0.0027
2025-10-19 13:26:31 - tokenizer_training_rank_0 - INFO - 
--- Epoch 18/30 Summary ---
Validation Loss: 0.0032
Epoch Time: 0:00:29
Total Training Time: 0:00:29

2025-10-19 13:26:34 - tokenizer_training_rank_0 - INFO - [Epoch 19/30, Step 50/475] LR: 0.000072, Loss: -0.0322
2025-10-19 13:26:34 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0045
  - Recon Loss All: 0.0027
2025-10-19 13:26:37 - tokenizer_training_rank_0 - INFO - [Epoch 19/30, Step 100/475] LR: 0.000071, Loss: -0.0321
2025-10-19 13:26:37 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0046
  - Recon Loss All: 0.0028
2025-10-19 13:26:41 - tokenizer_training_rank_0 - INFO - [Epoch 19/30, Step 150/475] LR: 0.000070, Loss: -0.0322
2025-10-19 13:26:41 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0045
  - Recon Loss All: 0.0027
2025-10-19 13:26:44 - tokenizer_training_rank_0 - INFO - [Epoch 19/30, Step 200/475] LR: 0.000068, Loss: -0.0321
2025-10-19 13:26:44 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0046
  - Recon Loss All: 0.0028
2025-10-19 13:26:47 - tokenizer_training_rank_0 - INFO - [Epoch 19/30, Step 250/475] LR: 0.000067, Loss: -0.0324
2025-10-19 13:26:47 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0717
  - Recon Loss Pre: 0.0043
  - Recon Loss All: 0.0026
2025-10-19 13:26:50 - tokenizer_training_rank_0 - INFO - [Epoch 19/30, Step 300/475] LR: 0.000066, Loss: -0.0321
2025-10-19 13:26:50 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0046
  - Recon Loss All: 0.0028
2025-10-19 13:26:53 - tokenizer_training_rank_0 - INFO - [Epoch 19/30, Step 350/475] LR: 0.000065, Loss: -0.0323
2025-10-19 13:26:53 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0717
  - Recon Loss Pre: 0.0044
  - Recon Loss All: 0.0027
2025-10-19 13:26:56 - tokenizer_training_rank_0 - INFO - [Epoch 19/30, Step 400/475] LR: 0.000064, Loss: -0.0323
2025-10-19 13:26:56 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0717
  - Recon Loss Pre: 0.0044
  - Recon Loss All: 0.0027
2025-10-19 13:26:58 - tokenizer_training_rank_0 - INFO - [Epoch 19/30, Step 450/475] LR: 0.000063, Loss: -0.0324
2025-10-19 13:26:58 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0717
  - Recon Loss Pre: 0.0042
  - Recon Loss All: 0.0027
2025-10-19 13:27:01 - tokenizer_training_rank_0 - INFO - 
--- Epoch 19/30 Summary ---
Validation Loss: 0.0032
Epoch Time: 0:00:30
Total Training Time: 0:00:30

2025-10-19 13:27:02 - tokenizer_training_rank_0 - INFO - [Epoch 20/30, Step 25/475] LR: 0.000062, Loss: -0.0322
2025-10-19 13:27:02 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0717
  - Recon Loss Pre: 0.0045
  - Recon Loss All: 0.0028
2025-10-19 13:27:05 - tokenizer_training_rank_0 - INFO - [Epoch 20/30, Step 75/475] LR: 0.000061, Loss: -0.0321
2025-10-19 13:27:05 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0046
  - Recon Loss All: 0.0028
2025-10-19 13:27:08 - tokenizer_training_rank_0 - INFO - [Epoch 20/30, Step 125/475] LR: 0.000060, Loss: -0.0321
2025-10-19 13:27:08 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0045
  - Recon Loss All: 0.0028
2025-10-19 13:27:11 - tokenizer_training_rank_0 - INFO - [Epoch 20/30, Step 175/475] LR: 0.000059, Loss: -0.0322
2025-10-19 13:27:11 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0045
  - Recon Loss All: 0.0027
2025-10-19 13:27:14 - tokenizer_training_rank_0 - INFO - [Epoch 20/30, Step 225/475] LR: 0.000058, Loss: -0.0319
2025-10-19 13:27:14 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0715
  - Recon Loss Pre: 0.0049
  - Recon Loss All: 0.0028
2025-10-19 13:27:17 - tokenizer_training_rank_0 - INFO - [Epoch 20/30, Step 275/475] LR: 0.000057, Loss: -0.0322
2025-10-19 13:27:17 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0045
  - Recon Loss All: 0.0028
2025-10-19 13:27:20 - tokenizer_training_rank_0 - INFO - [Epoch 20/30, Step 325/475] LR: 0.000056, Loss: -0.0322
2025-10-19 13:27:20 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0045
  - Recon Loss All: 0.0027
2025-10-19 13:27:22 - tokenizer_training_rank_0 - INFO - [Epoch 20/30, Step 375/475] LR: 0.000055, Loss: -0.0322
2025-10-19 13:27:22 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0044
  - Recon Loss All: 0.0027
2025-10-19 13:27:25 - tokenizer_training_rank_0 - INFO - [Epoch 20/30, Step 425/475] LR: 0.000054, Loss: -0.0322
2025-10-19 13:27:25 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0044
  - Recon Loss All: 0.0027
2025-10-19 13:27:28 - tokenizer_training_rank_0 - INFO - [Epoch 20/30, Step 475/475] LR: 0.000053, Loss: -0.0323
2025-10-19 13:27:28 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0044
  - Recon Loss All: 0.0026
2025-10-19 13:27:29 - tokenizer_training_rank_0 - INFO - 
--- Epoch 20/30 Summary ---
Validation Loss: 0.0032
Epoch Time: 0:00:28
Total Training Time: 0:00:28

2025-10-19 13:27:33 - tokenizer_training_rank_0 - INFO - [Epoch 21/30, Step 50/475] LR: 0.000052, Loss: -0.0322
2025-10-19 13:27:33 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0045
  - Recon Loss All: 0.0027
2025-10-19 13:27:36 - tokenizer_training_rank_0 - INFO - [Epoch 21/30, Step 100/475] LR: 0.000051, Loss: -0.0322
2025-10-19 13:27:36 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0046
  - Recon Loss All: 0.0027
2025-10-19 13:27:39 - tokenizer_training_rank_0 - INFO - [Epoch 21/30, Step 150/475] LR: 0.000050, Loss: -0.0321
2025-10-19 13:27:39 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0046
  - Recon Loss All: 0.0029
2025-10-19 13:27:42 - tokenizer_training_rank_0 - INFO - [Epoch 21/30, Step 200/475] LR: 0.000049, Loss: -0.0322
2025-10-19 13:27:42 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0044
  - Recon Loss All: 0.0028
2025-10-19 13:27:45 - tokenizer_training_rank_0 - INFO - [Epoch 21/30, Step 250/475] LR: 0.000048, Loss: -0.0321
2025-10-19 13:27:45 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0046
  - Recon Loss All: 0.0028
2025-10-19 13:27:48 - tokenizer_training_rank_0 - INFO - [Epoch 21/30, Step 300/475] LR: 0.000047, Loss: -0.0321
2025-10-19 13:27:48 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0046
  - Recon Loss All: 0.0028
2025-10-19 13:27:51 - tokenizer_training_rank_0 - INFO - [Epoch 21/30, Step 350/475] LR: 0.000046, Loss: -0.0324
2025-10-19 13:27:51 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0042
  - Recon Loss All: 0.0026
2025-10-19 13:27:53 - tokenizer_training_rank_0 - INFO - [Epoch 21/30, Step 400/475] LR: 0.000045, Loss: -0.0322
2025-10-19 13:27:54 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0044
  - Recon Loss All: 0.0027
2025-10-19 13:27:56 - tokenizer_training_rank_0 - INFO - [Epoch 21/30, Step 450/475] LR: 0.000044, Loss: -0.0322
2025-10-19 13:27:56 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0045
  - Recon Loss All: 0.0027
2025-10-19 13:27:59 - tokenizer_training_rank_0 - INFO - 
--- Epoch 21/30 Summary ---
Validation Loss: 0.0031
Epoch Time: 0:00:29
Total Training Time: 0:00:29

2025-10-19 13:28:01 - tokenizer_training_rank_0 - INFO - [Epoch 22/30, Step 25/475] LR: 0.000043, Loss: -0.0324
2025-10-19 13:28:01 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0042
  - Recon Loss All: 0.0026
2025-10-19 13:28:03 - tokenizer_training_rank_0 - INFO - [Epoch 22/30, Step 75/475] LR: 0.000042, Loss: -0.0323
2025-10-19 13:28:03 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0044
  - Recon Loss All: 0.0027
2025-10-19 13:28:06 - tokenizer_training_rank_0 - INFO - [Epoch 22/30, Step 125/475] LR: 0.000041, Loss: -0.0321
2025-10-19 13:28:06 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0047
  - Recon Loss All: 0.0027
2025-10-19 13:28:09 - tokenizer_training_rank_0 - INFO - [Epoch 22/30, Step 175/475] LR: 0.000040, Loss: -0.0321
2025-10-19 13:28:09 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0045
  - Recon Loss All: 0.0028
2025-10-19 13:28:12 - tokenizer_training_rank_0 - INFO - [Epoch 22/30, Step 225/475] LR: 0.000039, Loss: -0.0321
2025-10-19 13:28:12 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0045
  - Recon Loss All: 0.0028
2025-10-19 13:28:15 - tokenizer_training_rank_0 - INFO - [Epoch 22/30, Step 275/475] LR: 0.000039, Loss: -0.0322
2025-10-19 13:28:15 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0045
  - Recon Loss All: 0.0028
2025-10-19 13:28:18 - tokenizer_training_rank_0 - INFO - [Epoch 22/30, Step 325/475] LR: 0.000038, Loss: -0.0321
2025-10-19 13:28:18 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0047
  - Recon Loss All: 0.0028
2025-10-19 13:28:20 - tokenizer_training_rank_0 - INFO - [Epoch 22/30, Step 375/475] LR: 0.000037, Loss: -0.0324
2025-10-19 13:28:20 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0043
  - Recon Loss All: 0.0025
2025-10-19 13:28:23 - tokenizer_training_rank_0 - INFO - [Epoch 22/30, Step 425/475] LR: 0.000036, Loss: -0.0321
2025-10-19 13:28:23 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0046
  - Recon Loss All: 0.0028
2025-10-19 13:28:26 - tokenizer_training_rank_0 - INFO - [Epoch 22/30, Step 475/475] LR: 0.000035, Loss: -0.0321
2025-10-19 13:28:26 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0046
  - Recon Loss All: 0.0029
2025-10-19 13:28:27 - tokenizer_training_rank_0 - INFO - 
--- Epoch 22/30 Summary ---
Validation Loss: 0.0032
Epoch Time: 0:00:28
Total Training Time: 0:00:28

2025-10-19 13:28:31 - tokenizer_training_rank_0 - INFO - [Epoch 23/30, Step 50/475] LR: 0.000034, Loss: -0.0321
2025-10-19 13:28:31 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0046
  - Recon Loss All: 0.0028
2025-10-19 13:28:34 - tokenizer_training_rank_0 - INFO - [Epoch 23/30, Step 100/475] LR: 0.000033, Loss: -0.0321
2025-10-19 13:28:34 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0046
  - Recon Loss All: 0.0027
2025-10-19 13:28:37 - tokenizer_training_rank_0 - INFO - [Epoch 23/30, Step 150/475] LR: 0.000032, Loss: -0.0322
2025-10-19 13:28:37 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0044
  - Recon Loss All: 0.0028
2025-10-19 13:28:41 - tokenizer_training_rank_0 - INFO - [Epoch 23/30, Step 200/475] LR: 0.000032, Loss: -0.0322
2025-10-19 13:28:41 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0046
  - Recon Loss All: 0.0027
2025-10-19 13:28:44 - tokenizer_training_rank_0 - INFO - [Epoch 23/30, Step 250/475] LR: 0.000031, Loss: -0.0323
2025-10-19 13:28:44 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0044
  - Recon Loss All: 0.0026
2025-10-19 13:28:47 - tokenizer_training_rank_0 - INFO - [Epoch 23/30, Step 300/475] LR: 0.000030, Loss: -0.0321
2025-10-19 13:28:47 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0045
  - Recon Loss All: 0.0028
2025-10-19 13:28:50 - tokenizer_training_rank_0 - INFO - [Epoch 23/30, Step 350/475] LR: 0.000029, Loss: -0.0321
2025-10-19 13:28:50 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0047
  - Recon Loss All: 0.0028
2025-10-19 13:28:53 - tokenizer_training_rank_0 - INFO - [Epoch 23/30, Step 400/475] LR: 0.000028, Loss: -0.0322
2025-10-19 13:28:53 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0045
  - Recon Loss All: 0.0027
2025-10-19 13:28:56 - tokenizer_training_rank_0 - INFO - [Epoch 23/30, Step 450/475] LR: 0.000028, Loss: -0.0321
2025-10-19 13:28:56 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0046
  - Recon Loss All: 0.0028
2025-10-19 13:28:58 - tokenizer_training_rank_0 - INFO - 
--- Epoch 23/30 Summary ---
Validation Loss: 0.0031
Epoch Time: 0:00:30
Total Training Time: 0:00:30

2025-10-19 13:29:00 - tokenizer_training_rank_0 - INFO - [Epoch 24/30, Step 25/475] LR: 0.000027, Loss: -0.0322
2025-10-19 13:29:00 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0045
  - Recon Loss All: 0.0027
2025-10-19 13:29:03 - tokenizer_training_rank_0 - INFO - [Epoch 24/30, Step 75/475] LR: 0.000026, Loss: -0.0323
2025-10-19 13:29:03 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0044
  - Recon Loss All: 0.0026
2025-10-19 13:29:06 - tokenizer_training_rank_0 - INFO - [Epoch 24/30, Step 125/475] LR: 0.000025, Loss: -0.0321
2025-10-19 13:29:06 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0046
  - Recon Loss All: 0.0028
2025-10-19 13:29:09 - tokenizer_training_rank_0 - INFO - [Epoch 24/30, Step 175/475] LR: 0.000025, Loss: -0.0320
2025-10-19 13:29:09 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0047
  - Recon Loss All: 0.0028
2025-10-19 13:29:13 - tokenizer_training_rank_0 - INFO - [Epoch 24/30, Step 225/475] LR: 0.000024, Loss: -0.0321
2025-10-19 13:29:13 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0047
  - Recon Loss All: 0.0028
2025-10-19 13:29:16 - tokenizer_training_rank_0 - INFO - [Epoch 24/30, Step 275/475] LR: 0.000023, Loss: -0.0321
2025-10-19 13:29:16 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0046
  - Recon Loss All: 0.0027
2025-10-19 13:29:19 - tokenizer_training_rank_0 - INFO - [Epoch 24/30, Step 325/475] LR: 0.000022, Loss: -0.0321
2025-10-19 13:29:19 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0046
  - Recon Loss All: 0.0026
2025-10-19 13:29:22 - tokenizer_training_rank_0 - INFO - [Epoch 24/30, Step 375/475] LR: 0.000022, Loss: -0.0321
2025-10-19 13:29:22 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0046
  - Recon Loss All: 0.0028
2025-10-19 13:29:25 - tokenizer_training_rank_0 - INFO - [Epoch 24/30, Step 425/475] LR: 0.000021, Loss: -0.0323
2025-10-19 13:29:25 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0044
  - Recon Loss All: 0.0026
2025-10-19 13:29:28 - tokenizer_training_rank_0 - INFO - [Epoch 24/30, Step 475/475] LR: 0.000020, Loss: -0.0323
2025-10-19 13:29:28 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0044
  - Recon Loss All: 0.0027
2025-10-19 13:29:30 - tokenizer_training_rank_0 - INFO - 
--- Epoch 24/30 Summary ---
Validation Loss: 0.0031
Epoch Time: 0:00:31
Total Training Time: 0:00:31

2025-10-19 13:29:33 - tokenizer_training_rank_0 - INFO - [Epoch 25/30, Step 50/475] LR: 0.000020, Loss: -0.0321
2025-10-19 13:29:33 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0047
  - Recon Loss All: 0.0027
2025-10-19 13:29:36 - tokenizer_training_rank_0 - INFO - [Epoch 25/30, Step 100/475] LR: 0.000019, Loss: -0.0323
2025-10-19 13:29:36 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0044
  - Recon Loss All: 0.0026
2025-10-19 13:29:39 - tokenizer_training_rank_0 - INFO - [Epoch 25/30, Step 150/475] LR: 0.000018, Loss: -0.0320
2025-10-19 13:29:39 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0715
  - Recon Loss Pre: 0.0047
  - Recon Loss All: 0.0028
2025-10-19 13:29:43 - tokenizer_training_rank_0 - INFO - [Epoch 25/30, Step 200/475] LR: 0.000018, Loss: -0.0323
2025-10-19 13:29:43 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0043
  - Recon Loss All: 0.0027
2025-10-19 13:29:46 - tokenizer_training_rank_0 - INFO - [Epoch 25/30, Step 250/475] LR: 0.000017, Loss: -0.0321
2025-10-19 13:29:46 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0715
  - Recon Loss Pre: 0.0046
  - Recon Loss All: 0.0028
2025-10-19 13:29:49 - tokenizer_training_rank_0 - INFO - [Epoch 25/30, Step 300/475] LR: 0.000016, Loss: -0.0324
2025-10-19 13:29:49 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0043
  - Recon Loss All: 0.0026
2025-10-19 13:29:53 - tokenizer_training_rank_0 - INFO - [Epoch 25/30, Step 350/475] LR: 0.000016, Loss: -0.0321
2025-10-19 13:29:53 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0046
  - Recon Loss All: 0.0028
2025-10-19 13:29:56 - tokenizer_training_rank_0 - INFO - [Epoch 25/30, Step 400/475] LR: 0.000015, Loss: -0.0324
2025-10-19 13:29:56 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0043
  - Recon Loss All: 0.0026
2025-10-19 13:29:59 - tokenizer_training_rank_0 - INFO - [Epoch 25/30, Step 450/475] LR: 0.000015, Loss: -0.0323
2025-10-19 13:29:59 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0044
  - Recon Loss All: 0.0026
2025-10-19 13:30:02 - tokenizer_training_rank_0 - INFO - 
--- Epoch 25/30 Summary ---
Validation Loss: 0.0031
Epoch Time: 0:00:32
Total Training Time: 0:00:32

2025-10-19 13:30:04 - tokenizer_training_rank_0 - INFO - [Epoch 26/30, Step 25/475] LR: 0.000014, Loss: -0.0322
2025-10-19 13:30:04 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0045
  - Recon Loss All: 0.0027
2025-10-19 13:30:07 - tokenizer_training_rank_0 - INFO - [Epoch 26/30, Step 75/475] LR: 0.000013, Loss: -0.0321
2025-10-19 13:30:07 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0045
  - Recon Loss All: 0.0028
2025-10-19 13:30:10 - tokenizer_training_rank_0 - INFO - [Epoch 26/30, Step 125/475] LR: 0.000013, Loss: -0.0320
2025-10-19 13:30:10 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0046
  - Recon Loss All: 0.0029
2025-10-19 13:30:14 - tokenizer_training_rank_0 - INFO - [Epoch 26/30, Step 175/475] LR: 0.000012, Loss: -0.0322
2025-10-19 13:30:14 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0044
  - Recon Loss All: 0.0027
2025-10-19 13:30:17 - tokenizer_training_rank_0 - INFO - [Epoch 26/30, Step 225/475] LR: 0.000012, Loss: -0.0318
2025-10-19 13:30:17 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0050
  - Recon Loss All: 0.0028
2025-10-19 13:30:20 - tokenizer_training_rank_0 - INFO - [Epoch 26/30, Step 275/475] LR: 0.000011, Loss: -0.0322
2025-10-19 13:30:20 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0047
  - Recon Loss All: 0.0026
2025-10-19 13:30:23 - tokenizer_training_rank_0 - INFO - [Epoch 26/30, Step 325/475] LR: 0.000011, Loss: -0.0322
2025-10-19 13:30:23 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0046
  - Recon Loss All: 0.0027
2025-10-19 13:30:27 - tokenizer_training_rank_0 - INFO - [Epoch 26/30, Step 375/475] LR: 0.000010, Loss: -0.0324
2025-10-19 13:30:27 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0717
  - Recon Loss Pre: 0.0042
  - Recon Loss All: 0.0026
2025-10-19 13:30:30 - tokenizer_training_rank_0 - INFO - [Epoch 26/30, Step 425/475] LR: 0.000010, Loss: -0.0323
2025-10-19 13:30:30 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0043
  - Recon Loss All: 0.0026
2025-10-19 13:30:33 - tokenizer_training_rank_0 - INFO - [Epoch 26/30, Step 475/475] LR: 0.000009, Loss: -0.0321
2025-10-19 13:30:33 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0046
  - Recon Loss All: 0.0027
2025-10-19 13:30:34 - tokenizer_training_rank_0 - INFO - 
--- Epoch 26/30 Summary ---
Validation Loss: 0.0031
Epoch Time: 0:00:32
Total Training Time: 0:00:32

2025-10-19 13:30:37 - tokenizer_training_rank_0 - INFO - [Epoch 27/30, Step 50/475] LR: 0.000009, Loss: -0.0322
2025-10-19 13:30:37 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0044
  - Recon Loss All: 0.0027
2025-10-19 13:30:41 - tokenizer_training_rank_0 - INFO - [Epoch 27/30, Step 100/475] LR: 0.000008, Loss: -0.0322
2025-10-19 13:30:41 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0044
  - Recon Loss All: 0.0027
2025-10-19 13:30:44 - tokenizer_training_rank_0 - INFO - [Epoch 27/30, Step 150/475] LR: 0.000008, Loss: -0.0324
2025-10-19 13:30:44 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0043
  - Recon Loss All: 0.0026
2025-10-19 13:30:47 - tokenizer_training_rank_0 - INFO - [Epoch 27/30, Step 200/475] LR: 0.000007, Loss: -0.0322
2025-10-19 13:30:47 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0045
  - Recon Loss All: 0.0027
2025-10-19 13:30:50 - tokenizer_training_rank_0 - INFO - [Epoch 27/30, Step 250/475] LR: 0.000007, Loss: -0.0322
2025-10-19 13:30:50 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0045
  - Recon Loss All: 0.0027
2025-10-19 13:30:52 - tokenizer_training_rank_0 - INFO - [Epoch 27/30, Step 300/475] LR: 0.000007, Loss: -0.0321
2025-10-19 13:30:52 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0047
  - Recon Loss All: 0.0026
2025-10-19 13:30:55 - tokenizer_training_rank_0 - INFO - [Epoch 27/30, Step 350/475] LR: 0.000006, Loss: -0.0324
2025-10-19 13:30:55 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0042
  - Recon Loss All: 0.0026
2025-10-19 13:30:58 - tokenizer_training_rank_0 - INFO - [Epoch 27/30, Step 400/475] LR: 0.000006, Loss: -0.0321
2025-10-19 13:30:58 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0046
  - Recon Loss All: 0.0027
2025-10-19 13:31:02 - tokenizer_training_rank_0 - INFO - [Epoch 27/30, Step 450/475] LR: 0.000005, Loss: -0.0321
2025-10-19 13:31:02 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0046
  - Recon Loss All: 0.0027
2025-10-19 13:31:04 - tokenizer_training_rank_0 - INFO - 
--- Epoch 27/30 Summary ---
Validation Loss: 0.0031
Epoch Time: 0:00:30
Total Training Time: 0:00:30

2025-10-19 13:31:06 - tokenizer_training_rank_0 - INFO - [Epoch 28/30, Step 25/475] LR: 0.000005, Loss: -0.0324
2025-10-19 13:31:06 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0042
  - Recon Loss All: 0.0026
2025-10-19 13:31:09 - tokenizer_training_rank_0 - INFO - [Epoch 28/30, Step 75/475] LR: 0.000005, Loss: -0.0323
2025-10-19 13:31:09 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0043
  - Recon Loss All: 0.0026
2025-10-19 13:31:13 - tokenizer_training_rank_0 - INFO - [Epoch 28/30, Step 125/475] LR: 0.000004, Loss: -0.0322
2025-10-19 13:31:13 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0044
  - Recon Loss All: 0.0027
2025-10-19 13:31:16 - tokenizer_training_rank_0 - INFO - [Epoch 28/30, Step 175/475] LR: 0.000004, Loss: -0.0323
2025-10-19 13:31:16 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0044
  - Recon Loss All: 0.0026
2025-10-19 13:31:19 - tokenizer_training_rank_0 - INFO - [Epoch 28/30, Step 225/475] LR: 0.000004, Loss: -0.0323
2025-10-19 13:31:19 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0043
  - Recon Loss All: 0.0027
2025-10-19 13:31:23 - tokenizer_training_rank_0 - INFO - [Epoch 28/30, Step 275/475] LR: 0.000003, Loss: -0.0322
2025-10-19 13:31:23 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0045
  - Recon Loss All: 0.0027
2025-10-19 13:31:26 - tokenizer_training_rank_0 - INFO - [Epoch 28/30, Step 325/475] LR: 0.000003, Loss: -0.0323
2025-10-19 13:31:26 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0043
  - Recon Loss All: 0.0026
2025-10-19 13:31:29 - tokenizer_training_rank_0 - INFO - [Epoch 28/30, Step 375/475] LR: 0.000003, Loss: -0.0323
2025-10-19 13:31:29 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0044
  - Recon Loss All: 0.0026
2025-10-19 13:31:32 - tokenizer_training_rank_0 - INFO - [Epoch 28/30, Step 425/475] LR: 0.000003, Loss: -0.0322
2025-10-19 13:31:32 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0044
  - Recon Loss All: 0.0027
2025-10-19 13:31:36 - tokenizer_training_rank_0 - INFO - [Epoch 28/30, Step 475/475] LR: 0.000002, Loss: -0.0318
2025-10-19 13:31:36 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0715
  - Recon Loss Pre: 0.0050
  - Recon Loss All: 0.0029
2025-10-19 13:31:37 - tokenizer_training_rank_0 - INFO - 
--- Epoch 28/30 Summary ---
Validation Loss: 0.0031
Epoch Time: 0:00:32
Total Training Time: 0:00:32

2025-10-19 13:31:40 - tokenizer_training_rank_0 - INFO - [Epoch 29/30, Step 50/475] LR: 0.000002, Loss: -0.0323
2025-10-19 13:31:40 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0043
  - Recon Loss All: 0.0027
2025-10-19 13:31:43 - tokenizer_training_rank_0 - INFO - [Epoch 29/30, Step 100/475] LR: 0.000002, Loss: -0.0324
2025-10-19 13:31:43 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0042
  - Recon Loss All: 0.0025
2025-10-19 13:31:47 - tokenizer_training_rank_0 - INFO - [Epoch 29/30, Step 150/475] LR: 0.000002, Loss: -0.0323
2025-10-19 13:31:47 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0045
  - Recon Loss All: 0.0026
2025-10-19 13:31:50 - tokenizer_training_rank_0 - INFO - [Epoch 29/30, Step 200/475] LR: 0.000001, Loss: -0.0318
2025-10-19 13:31:50 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0715
  - Recon Loss Pre: 0.0049
  - Recon Loss All: 0.0030
2025-10-19 13:31:54 - tokenizer_training_rank_0 - INFO - [Epoch 29/30, Step 250/475] LR: 0.000001, Loss: -0.0323
2025-10-19 13:31:54 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0044
  - Recon Loss All: 0.0027
2025-10-19 13:31:57 - tokenizer_training_rank_0 - INFO - [Epoch 29/30, Step 300/475] LR: 0.000001, Loss: -0.0322
2025-10-19 13:31:57 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0045
  - Recon Loss All: 0.0027
2025-10-19 13:32:01 - tokenizer_training_rank_0 - INFO - [Epoch 29/30, Step 350/475] LR: 0.000001, Loss: -0.0322
2025-10-19 13:32:01 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0045
  - Recon Loss All: 0.0027
2025-10-19 13:32:04 - tokenizer_training_rank_0 - INFO - [Epoch 29/30, Step 400/475] LR: 0.000001, Loss: -0.0323
2025-10-19 13:32:04 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0044
  - Recon Loss All: 0.0026
2025-10-19 13:32:07 - tokenizer_training_rank_0 - INFO - [Epoch 29/30, Step 450/475] LR: 0.000001, Loss: -0.0321
2025-10-19 13:32:07 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0046
  - Recon Loss All: 0.0027
2025-10-19 13:32:10 - tokenizer_training_rank_0 - INFO - 
--- Epoch 29/30 Summary ---
Validation Loss: 0.0031
Epoch Time: 0:00:32
Total Training Time: 0:00:32

2025-10-19 13:32:11 - tokenizer_training_rank_0 - INFO - [Epoch 30/30, Step 25/475] LR: 0.000001, Loss: -0.0323
2025-10-19 13:32:11 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0044
  - Recon Loss All: 0.0026
2025-10-19 13:32:15 - tokenizer_training_rank_0 - INFO - [Epoch 30/30, Step 75/475] LR: 0.000000, Loss: -0.0321
2025-10-19 13:32:15 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0047
  - Recon Loss All: 0.0028
2025-10-19 13:32:18 - tokenizer_training_rank_0 - INFO - [Epoch 30/30, Step 125/475] LR: 0.000000, Loss: -0.0323
2025-10-19 13:32:18 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0044
  - Recon Loss All: 0.0026
2025-10-19 13:32:21 - tokenizer_training_rank_0 - INFO - [Epoch 30/30, Step 175/475] LR: 0.000000, Loss: -0.0322
2025-10-19 13:32:21 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0044
  - Recon Loss All: 0.0027
2025-10-19 13:32:25 - tokenizer_training_rank_0 - INFO - [Epoch 30/30, Step 225/475] LR: 0.000000, Loss: -0.0321
2025-10-19 13:32:25 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0049
  - Recon Loss All: 0.0026
2025-10-19 13:32:28 - tokenizer_training_rank_0 - INFO - [Epoch 30/30, Step 275/475] LR: 0.000000, Loss: -0.0321
2025-10-19 13:32:28 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0046
  - Recon Loss All: 0.0027
2025-10-19 13:32:31 - tokenizer_training_rank_0 - INFO - [Epoch 30/30, Step 325/475] LR: 0.000000, Loss: -0.0323
2025-10-19 13:32:31 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0043
  - Recon Loss All: 0.0026
2025-10-19 13:32:34 - tokenizer_training_rank_0 - INFO - [Epoch 30/30, Step 375/475] LR: 0.000000, Loss: -0.0323
2025-10-19 13:32:35 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0043
  - Recon Loss All: 0.0027
2025-10-19 13:32:38 - tokenizer_training_rank_0 - INFO - [Epoch 30/30, Step 425/475] LR: 0.000000, Loss: -0.0322
2025-10-19 13:32:38 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0044
  - Recon Loss All: 0.0027
2025-10-19 13:32:41 - tokenizer_training_rank_0 - INFO - [Epoch 30/30, Step 475/475] LR: 0.000000, Loss: -0.0321
2025-10-19 13:32:41 - tokenizer_training_rank_0 - INFO -   - VQ Loss: -0.0716
  - Recon Loss Pre: 0.0045
  - Recon Loss All: 0.0028
2025-10-19 13:32:42 - tokenizer_training_rank_0 - INFO - 
--- Epoch 30/30 Summary ---
Validation Loss: 0.0031
Epoch Time: 0:00:32
Total Training Time: 0:00:32

2025-10-19 13:32:42 - tokenizer_training_rank_0 - INFO - Tokenizer training completed! Best validation loss: 0.0030
Training time: 15.87 minutes
Model saved to: /root/project/Kronos-Btc-finetune-master/Kronos/finetune_csv/finetuned//BTCUSDT_1h_finetune/tokenizer
